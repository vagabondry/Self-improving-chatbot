{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:17:13.407643Z",
     "iopub.status.busy": "2024-11-23T15:17:13.407206Z",
     "iopub.status.idle": "2024-11-23T15:17:25.573260Z",
     "shell.execute_reply": "2024-11-23T15:17:25.572205Z",
     "shell.execute_reply.started": "2024-11-23T15:17:13.407592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_scheduler, pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:15:42.897642Z",
     "iopub.status.busy": "2024-11-23T15:15:42.897196Z",
     "iopub.status.idle": "2024-11-23T15:15:43.241168Z",
     "shell.execute_reply": "2024-11-23T15:15:43.240097Z",
     "shell.execute_reply.started": "2024-11-23T15:15:42.897595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                                  0  \\\n",
      "0           0            What kind of phone(s) do you guys have?   \n",
      "1           1  I have a pixel. It's pretty great. Much better...   \n",
      "2           2       Does it really charge all the way in 15 min?   \n",
      "3           3            What kind of phone(s) do you guys have?   \n",
      "4           4  Samsung Galaxy J1. It's my first cell phone an...   \n",
      "\n",
      "                                                   1  \\\n",
      "0  I have a pixel. It's pretty great. Much better...   \n",
      "1       Does it really charge all the way in 15 min?   \n",
      "2  Pretty fast. I've never timed it, but it's und...   \n",
      "3  Samsung Galaxy J1. It's my first cell phone an...   \n",
      "4  What do you think of it? Anything you don't like?   \n",
      "\n",
      "                                                   2  \n",
      "0       Does it really charge all the way in 15 min?  \n",
      "1  Pretty fast. I've never timed it, but it's und...  \n",
      "2  cool. I've been thinking of getting one, my ph...  \n",
      "3  What do you think of it? Anything you don't like?  \n",
      "4  I love it. I can't think of anything I don't l...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/casual_data_windows.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:17:43.350120Z",
     "iopub.status.busy": "2024-11-23T15:17:43.349096Z",
     "iopub.status.idle": "2024-11-23T15:26:36.740792Z",
     "shell.execute_reply": "2024-11-23T15:26:36.739691Z",
     "shell.execute_reply.started": "2024-11-23T15:17:43.350072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85d24027fe34d8fb771ff943e197495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e7452b6e7b4c1292d03ab39e78536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f1109f11c446d7b2e968e4735f659d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b555db411f4841ac6eb27366115b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34731528d1a146f0aa6c864c7d0b397f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553df81f23b44fe487489eae211d16bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6cd19c285747ef95c8bf59b3197728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [NEUTRAL] What kind of phone(s) do you guys ha...\n",
      "1    [NEUTRAL] I have a pixel. It's pretty great. M...\n",
      "2    [NEUTRAL] Does it really charge all the way in...\n",
      "3    [NEUTRAL] What kind of phone(s) do you guys ha...\n",
      "4    [NEUTRAL] Samsung Galaxy J1. It's my first cel...\n",
      "Name: formatted_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "desired_labels = {'neutral', 'disapproval', 'caring', 'annoyance', 'anger', 'excitement', 'joy'}\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None, device=\"cuda\")\n",
    "\n",
    "def format_conversation(row, classifier):\n",
    "    # Format the conversation string\n",
    "    if pd.notna(row['2']):\n",
    "        formatted = f\"{row['0']} [SEP] {row['1']} [SEP] {row['2']}\"\n",
    "    else:\n",
    "        formatted = f\"{row['0']} [SEP] {row['1']}\"\n",
    "\n",
    "    model_outputs = classifier(formatted)\n",
    "    model_outputs = model_outputs[0]\n",
    "\n",
    "    # Filter outputs for desired emotion labels\n",
    "    filtered_outputs = [output for output in model_outputs if output['label'] in desired_labels]\n",
    "    \n",
    "    # Extract labels with scores > 0.2\n",
    "    relevant_emotions = [output['label'].upper() for output in filtered_outputs if output['score'] > 0.2]\n",
    "    \n",
    "    # Default to NEUTRAL if no emotion scores are above threshold\n",
    "    if not relevant_emotions:\n",
    "        relevant_emotions = [\"NEUTRAL\"]\n",
    "    \n",
    "    # Combine emotion tokens with the formatted conversation\n",
    "    emotion_tokens = \" \".join([f\"[{emotion}]\" for emotion in relevant_emotions])\n",
    "    return f\"{emotion_tokens} {formatted}\"\n",
    "\n",
    "df['formatted_text'] = df.apply(lambda row: format_conversation(row, classifier), axis=1)\n",
    "\n",
    "# Preview the dataset\n",
    "print(df['formatted_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:27:34.670451Z",
     "iopub.status.busy": "2024-11-23T15:27:34.670041Z",
     "iopub.status.idle": "2024-11-23T15:27:34.677329Z",
     "shell.execute_reply": "2024-11-23T15:27:34.676333Z",
     "shell.execute_reply.started": "2024-11-23T15:27:34.670401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, conversations, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.conversations = conversations\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.conversations[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'labels': encoded['input_ids'].squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:28:18.416297Z",
     "iopub.status.busy": "2024-11-23T15:28:18.415452Z",
     "iopub.status.idle": "2024-11-23T15:28:18.625836Z",
     "shell.execute_reply": "2024-11-23T15:28:18.624908Z",
     "shell.execute_reply.started": "2024-11-23T15:28:18.416237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'sep_token': '[SEP]'})\n",
    "\n",
    "special_tokens = [\"[NEUTRAL]\", \"[DISAPPROVAL]\", \"[CARING]\", \"[ANNOYANCE]\", \"[ANGER]\", \"[EXCITEMENT]\", \"[JOY]\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:28:24.985005Z",
     "iopub.status.busy": "2024-11-23T15:28:24.984632Z",
     "iopub.status.idle": "2024-11-23T15:28:24.995786Z",
     "shell.execute_reply": "2024-11-23T15:28:24.994715Z",
     "shell.execute_reply.started": "2024-11-23T15:28:24.984971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = ConversationDataset(dataset, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:28:29.997743Z",
     "iopub.status.busy": "2024-11-23T15:28:29.996971Z",
     "iopub.status.idle": "2024-11-23T15:28:33.979364Z",
     "shell.execute_reply": "2024-11-23T15:28:33.978289Z",
     "shell.execute_reply.started": "2024-11-23T15:28:29.997707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f3e97615941fb864197bcf8f93a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b88b28a3df43cda73d8b2ca747e17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:28:51.653573Z",
     "iopub.status.busy": "2024-11-23T15:28:51.652712Z",
     "iopub.status.idle": "2024-11-23T15:28:51.665638Z",
     "shell.execute_reply": "2024-11-23T15:28:51.664594Z",
     "shell.execute_reply.started": "2024-11-23T15:28:51.653518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "batch_size = 2\n",
    "warmup_steps = 100\n",
    "max_seq_len = 128\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * epochs\n",
    ")\n",
    "\n",
    "# Loss logging\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:29:01.320932Z",
     "iopub.status.busy": "2024-11-23T15:29:01.320526Z",
     "iopub.status.idle": "2024-11-23T16:39:45.948120Z",
     "shell.execute_reply": "2024-11-23T16:39:45.947140Z",
     "shell.execute_reply.started": "2024-11-23T15:29:01.320888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14075/14075 [23:41<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.131512241463043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14075/14075 [23:33<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.8778699633622042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14075/14075 [23:29<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7936823577334572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('conversation_gpt2_with_emotions/tokenizer_config.json',\n",
       " 'conversation_gpt2_with_emotions/special_tokens_map.json',\n",
       " 'conversation_gpt2_with_emotions/vocab.json',\n",
       " 'conversation_gpt2_with_emotions/merges.txt',\n",
       " 'conversation_gpt2_with_emotions/added_tokens.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"conversation_gpt2_with_emotions\")\n",
    "tokenizer.save_pretrained(\"conversation_gpt2_with_emotions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:03:41.033248Z",
     "iopub.status.busy": "2024-11-23T15:03:41.032328Z",
     "iopub.status.idle": "2024-11-23T15:03:41.476366Z",
     "shell.execute_reply": "2024-11-23T15:03:41.475355Z",
     "shell.execute_reply.started": "2024-11-23T15:03:41.033206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl this is crazy  You guys have amazing friends who love you  ðŸ˜€ðŸ˜Šâ˜ºðŸ™ŒðŸ‘ðŸ‘Š  ðŸ˜‚ðŸ˜‚ ðŸ˜†\n",
      "ðŸ¬¬ LOL How do\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_length=50, top_k=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"girl this is crazy [SEP]\"\n",
    "response = generate_response(prompt, model, tokenizer)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:10:26.526828Z",
     "iopub.status.busy": "2024-11-23T17:10:26.526099Z",
     "iopub.status.idle": "2024-11-23T17:10:27.727714Z",
     "shell.execute_reply": "2024-11-23T17:10:27.726815Z",
     "shell.execute_reply.started": "2024-11-23T17:10:26.526789Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger:  surely this will go so well  ~~you're right~~ there's nothing wrong with that.   Yup?\n",
      "\n",
      "excitement:  surely this will go so well   I hope so It is :w The good kind I'm\n",
      "\n",
      "joy:  surely this will go so well  ~~it'll go well~~  I still think so, and I'm glad I didn't\n",
      "\n",
      "neutral:  surely this will go so well   it will indeed  Itâ€™s not gonna happen, it just wonâ€™t\n",
      "\n",
      "disapproval:  surely this will go so well   This just made me get a little upset that my friend didn't reply so much as I needed\n",
      "\n",
      "caring:  surely this will go so well   I think he might be joking.  It will. Just remember that in the end the world\n",
      "\n",
      "annoyance:  surely this will go so well   I doubt it but it's ok :)  Ah! How long have you been treating yourself?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response_with_emotions(prompt, emotion, model, tokenizer, max_length=30, top_k=50):\n",
    "    model.eval()\n",
    "    prompt_with_emotion = f\"[{emotion.upper()}] {prompt}\"\n",
    "    input_ids = tokenizer.encode(prompt_with_emotion, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"surely this will go so well [SEP] \"\n",
    "emotions = {\"neutral\", \"disapproval\", \"caring\", \"annoyance\", \"anger\", \"excitement\", \"joy\"}\n",
    "for emotion in emotions:\n",
    "    response = generate_response_with_emotions(prompt, emotion, model, tokenizer)\n",
    "    print(emotion + \": \" + response + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:04:17.345620Z",
     "iopub.status.busy": "2024-11-23T15:04:17.345220Z",
     "iopub.status.idle": "2024-11-23T15:04:17.732096Z",
     "shell.execute_reply": "2024-11-23T15:04:17.731082Z",
     "shell.execute_reply.started": "2024-11-23T15:04:17.345588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Path to the saved model and tokenizer\n",
    "model_path = \"../model/conversation-gpt2-with-emotions\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 540134,
     "sourceId": 986563,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
